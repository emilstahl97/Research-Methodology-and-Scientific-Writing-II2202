\section{Research conclusion}
\label{sec:conclusion}
Data formats are a critical component of data science pipelines to ensure high performance. Since different data formats are implemented using different technologies, they may differ in how they handle different sizes of data, which affects the performance when processing and storing the data. This means that it is of interest of understanding the potential limitations of various data formats used in data science \cite{cao_data_2017}. This research has performed a set of benchmarks to reveal potential limitations in how various data formats perform in read/write speeds, resilience against potential file errors, and file storage size. The benchmarking methodology consisted of empirically and programmatically performing benchmarks with a set of Python scripts that iteratively executes the desired operations multiple times. The results were saved for further analysis and show that Parquet is the most suitable data format when it comes to handling textual time series data. This is because Parquet achieved the lowest read/write time as well as the smallest file storage size. However, due to the compressed nature of Parquet, it performed the worst in file stability benchmarks. Therefore, there exists a trade-off between read/write performance as well as file storage size and resilience against bit-flips that cause file errors. For future research purposes, one should consider running the same read/write benchmark but with Apache Spark instead of Pandas. Additionally, other applications rather than textual time series data may be researched to better understand the differences between the data formats when used in different data science applications. 