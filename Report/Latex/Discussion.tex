\clearpage
\section{Discussion}
\label{sec:discussion}
This section consists of two major sections that provide an analysis of the results presented in section \ref{sec:results} and respectively, an analysis regarding the different parts of the project as well as how they may have impacted the results.

\subsection{Analysis of results}
This section presents an analysis of the results shown in Section \ref{sec:results}.
\subsubsection{Read \& Write Benchmark}
For both the read and write benchmarks with the Pandas library for textual time series data, the Parquet format has the best performance. For reading, Parquet format read time only increases by around 0.04 seconds for every 5MB increase in file size, whereas the second format in place, the csv format, has an increase of around 0.25 seconds for every 5MB increase in file size. This is most likely due to the effective columnar storage style and the compression that is done with it. Interestingly, even thus the Avro file has the same compression codec, the file size is reduced but not as much as with the Parquet file format. For a discussion on file size and compression, see \ref{sec:filestoresizesummary}. The write performance of the Parquet format is astounding since the write time does not increase with the amount of data written. Parquet also manages to give the best compression rate compared to the rest of the file formats which is shown in Figure 6 \ref{fig:size_summary}. This makes it a clear winner in this benchmark. 


\subsubsection{File stability}
The data presented in Section \ref{sect:file-stability-results} clearly shows that the csv data format experience considerably fewer reading errors and undetected effects when compared to Avro and Parquet. This is potentially due to that csv is a very redundant and uncompressed data format, which means that the randomly flipped bit is unlikely to have any meaningful impact if the flipped bit happens on the metadata or the comma used as separation. For files with compression, there is a higher probability of flipping an important bit that needs to be used to recreate and read the data correctly \cite{vohra_apache_2016_avro, shafranovich_common_2005, cao_data_2017}. The results of the file stability benchmark show that the csv data format is more resilient against file errors and undetected effects due to bit-flip when compared with Avro and Parquet. Furthermore, when comparing the number of errors and undetected effects for Avro and Parquet it can be seen that Avro experiences more errors than Parquet. Regarding the cases where there were no file reading errors, Avro resulted in more undetected effects when compared to Parquet. Since Parquet performed the best in read/write and file size performance, our research shows that there is a trade-off between file stability and read/write performance as well as file size.  

\subsubsection{File Store Size}
\label{sec:filestoresizesummary}
As shown in Figure \ref{fig:size_summary}, xlsx suffers the most in size when converted from the csv format while both Avro and Parquet show a huge file size reduction. Parquet seems to use the time series data the best since it managed to compress the 20.91 GB file into only merely 917.6MB. This is less than \(\frac{1}{20}\) when compared to the original csv file, and less than \(\frac{1}{3}\) when compared with the Avro format. This means that Parquet is once again the preferred format to its columnar storage style and the snappy compression codec.

\subsection{Discussion}
Although Parquet scored the lowest point out of the 3 file formats that were benchmarked in the file stability benchmark, the columnar-based storage format that is Parquet seems to be the best fit for textual time-series data. It has the best file size compression and read \& write performance. Therefore, one can argue that the probability that a bit flip happens in a Parquet file transfer of 917.6 MB is less likely when compared to a transfer of a csv file with a size of 20.91 GB. Not to mention the savings on hard disk space as well as network bandwidth. One may argue that when it comes to big data analysis, it is implausible that people only use Pandas for data analysis, transformation, and visualization. Tools such as Apache Spark are more likely to be used in place. Furthermore, both Avro and Parquet formats support schema in their file format as well as schema evolution. Therefore, with Spark, we can more easily and effectively check for file corruption as well as get a valid schema from the file itself. This means that for future research purposes, one should consider running the same read/write and file stability benchmarks but with Apache Spark instead of Pandas \cite{vohra_apache_2016, cao_data_2017}.