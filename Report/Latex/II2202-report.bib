
@misc{radecic_stop_2021,
	title = {Stop {Using} {CSVs} for {Storage} — {This} {File} {Format} {Is} 150 {Times} {Faster}},
	url = {https://towardsdatascience.com/stop-using-csvs-for-storage-this-file-format-is-150-times-faster-158bd322074e},
	abstract = {CSV’s are costing you time, disk space, and money. It’s time to end it.},
	language = {en},
	urldate = {2022-09-03},
	journal = {Medium},
	author = {Radečić, Dario},
	month = sep,
	year = {2021},
	file = {Snapshot:C\:\\Users\\Steven\\Zotero\\storage\\36WR5R6B\\stop-using-csvs-for-storage-this-file-format-is-150-times-faster-158bd322074e.html:text/html},
}

@book{rabl_big_2016,
	address = {Cham},
	edition = {1st ed. 2016..},
	series = {Information {Systems} and {Applications}, incl. {Internet}/{Web}, and {HCI}},
	title = {Big {Data} {Benchmarking} 6th {International} {Workshop}, {WBDB} 2015, {Toronto}, {ON}, {Canada}, {June} 16-17, 2015 and 7th {International} {Workshop}, {WBDB} 2015, {New} {Delhi}, {India}, {December} 14-15, 2015, {Revised} {Selected} {Papers}},
	isbn = {978-3-319-49748-8},
	abstract = {Big Data, Simulations and HPC Convergence -- Benchmarking Fast-Data Platforms for the Aadhaar Biometric Database -- Towards a General Array Database Benchmark: Measuring Storage Access -- ALOJA: a Benchmarking and Predictive Platform for Big Data Performance Analysis -- A Set of Metrics to Evaluate HDFS and S3 Performance on Amazon EMR with Avro and Parquet Formats -- Benchmarking the Availability and Fault Tolerance of Cassandra -- Performance Evaluation of Spark SQL using BigBench -- Accelerating Big Bench on Hadoop., This book constitutes the thoroughly refereed post-workshop proceedings of the 6th International Workshop on Big Data Benchmarking, WBDB 2015, held in Toronto, ON, Canada, in June 2015 and the 7th International Workshop, WBDB 2015, held in New Delhi, India, in December 2015. The 8 full papers presented in this book were carefully reviewed and selected from 22 submissions. They deal with recent trends in big data and HPC convergence, new proposals for big data benchmarking, as well as tooling and performance results. .},
	language = {eng},
	number = {10044},
	publisher = {Springer International Publishing},
	author = {Rabl, Tilmann and Nambiar, Raghunath and Baru, Chaitanya and Bhandarkar, Milind and Poess, Meikel and Pyne, Saumyadipta},
	year = {2016},
}

@article{munir_cost-based_2019,
	title = {A cost-based storage format selector for materialized results in big data frameworks},
	volume = {38},
	issn = {0926-8782},
	doi = {10.1007/s10619-019-07271-0},
	abstract = {Modern big data frameworks (such as Hadoop and Spark) allow multiple users to do large-scale analysis simultaneously, by deploying data-intensive workflows (DIWs). These DIWs of different users share many common tasks (i.e, 50–80\%), which can be materialized and reused in future executions. Materializing the output of such common tasks improves the overall processing time of DIWs and also saves computational resources. Current solutions for materialization store data on Distributed File Systems by using a fixed storage format. However, a fixed choice is not the optimal one for every situation. Specifically, different layouts (i.e., horizontal, vertical or hybrid) have a huge impact on execution, according to the access patterns of the subsequent operations. In this paper, we present a cost-based approach that helps deciding the most appropriate storage format in every situation. A generic cost-based framework that selects the best format by considering the three main layouts is presented. Then, we use our framework to instantiate cost models for specific Hadoop storage formats (namely SequenceFile, Avro and Parquet), and test it with two standard benchmark suits. Our solution gives on average 1.33
×
speedup over fixed SequenceFile, 1.11
×
speedup over fixed Avro, 1.32
×
speedup over fixed Parquet, and overall, it provides 1.25
×
speedup.},
	language = {eng},
	number = {2},
	journal = {Distributed and parallel databases : an international journal},
	author = {Munir, Rana Faisal and Abelló, Alberto and Romero, Oscar and Thiele, Maik and Lehner, Wolfgang},
	year = {2019},
	note = {Place: New York
Publisher: Springer US},
	keywords = {Àrees temàtiques de la UPC, Article, Big Data, Computer Science, Cost model, Data Structures, Data-intensive workflows, Database Management, Emmagatzematge i recuperació de la informació, File organization (Computer science), Fitxers informàtics, Format, HDFS, Informàtica, Information Systems Applications (incl.Internet), Layouts, Macrodades, Materialized results, Memory Structures, Oganització, Operating Systems, Sistemes d'informació, Storage format},
	pages = {335--364},
	file = {Accepted Version:C\:\\Users\\Steven\\Zotero\\storage\\DN53HLK9\\Munir et al. - 2019 - A cost-based storage format selector for materiali.pdf:application/pdf},
}

@incollection{vohra_apache_2016,
	address = {Berkeley, CA},
	title = {Apache {Parquet}},
	isbn = {978-1-4842-2198-3},
	abstract = {Apache Parquet is an efficient, structured, column-oriented (also called columnar storage), compressed, binary file format. Parquet supports several compression codecs, including Snappy, GZIP, deflate, and BZIP2. Snappy is the default. Structured file formats such as RCFile, Avro, SequenceFile, and Parquet offer better performance with compression support, which reduces the size of the data on the disk and consequently the I/O and CPU resources required to deserialize data.},
	language = {eng},
	booktitle = {Practical {Hadoop} {Ecosystem}},
	publisher = {Apress},
	author = {Vohra, Deepak},
	year = {2016},
	doi = {10.1007/978-1-4842-2199-0_8},
	keywords = {Export HADOOP, File Format Structure, Hive External Table, Parquet, SequenceFile},
	pages = {325--335},
}

@article{plase_comparison_2017,
	title = {A {Comparison} of {HDFS} {Compact} {Data} {Formats}: {Avro} {Versus} {Parquet}},
	volume = {9},
	issn = {2029-2341},
	shorttitle = {A {Comparison} of {HDFS} {Compact} {Data} {Formats}},
	doi = {10.3846/mla.2017.1033},
	abstract = {In this paper, file formats like Avro and Parquet are compared with text formats to evaluate the performance of the data queries. Different data query patterns have been evaluated. Cloudera’s open-source Apache Hadoop distribution CDH 5.4 has been chosen for the experiments presented in this article. The results show that compact data formats (Avro and Parquet) take up less storage space when compared with plain text data formats because of binary data format and compression advantage. Furthermore, data queries from the column based data format Parquet are faster when compared with text data formats and Avro.},
	language = {eng},
	number = {3},
	journal = {Science future of Lithuania},
	author = {Plase, Daiga and Niedrite, Laila and Taranovs, Romans},
	year = {2017},
	note = {Place: Vilnius
Publisher: Gediminas Technical University},
	keywords = {Format, Big data, Binary data, Comparative studies, Compression, Data analysis, Data compression, Object oriented programming, Queries, Software reviews, Usage},
	pages = {267--276},
	file = {Full Text:C\:\\Users\\Steven\\Zotero\\storage\\74QER5M3\\Plase et al. - 2017 - A Comparison of HDFS Compact Data Formats Avro Ve.pdf:application/pdf},
}

@article{ivanov_impact_2020,
	title = {The impact of columnar file formats on {SQL}‐on‐hadoop engine performance: {A} study on {ORC} and {Parquet}},
	volume = {32},
	issn = {1532-0626},
	shorttitle = {The impact of columnar file formats on {SQL}‐on‐hadoop engine performance},
	doi = {10.1002/cpe.5523},
	abstract = {Summary
Columnar file formats provide an efficient way to store data to be queried by SQL‐on‐Hadoop engines. Related works consider the performance of processing engine and file format together, which makes it impossible to predict their individual impact. In this work, we propose an alternative approach: by executing each file format on the same processing engine, we compare the different file formats as well as their different parameter settings. We apply our strategy to two processing engines, Hive and SparkSQL, and evaluate the performance of two columnar file formats, ORC and Parquet. We use BigBench (TPCx‐BB), a standardized application‐level benchmark for Big Data scenarios. Our experiments confirm that the file format selection and its configuration significantly affect the overall performance. We show that ORC generally performs better on Hive, whereas Parquet achieves best performance with SparkSQL. Using ZLIB compression brings up to 60.2\% improvement with ORC, while Parquet achieves up to 7\% improvement with Snappy. Exceptions are the queries involving text processing, which do not benefit from using any compression.},
	language = {eng},
	number = {5},
	journal = {Concurrency and computation},
	author = {Ivanov, Todor and Pergolesi, Matteo},
	year = {2020},
	note = {Place: Hoboken
Publisher: Wiley Subscription Services, Inc},
	keywords = {Format, Parquet, Big data, Software reviews, big data benchmarking, BigBench, columnar file formats, Engines, Hive, ORC, Performance evaluation, Query languages, Query processing, Software upgrading, SparkSQL, SQL‐on‐Hadoop},
	pages = {n/a},
	file = {Full Text:C\:\\Users\\Steven\\Zotero\\storage\\VJBMHRTQ\\Ivanov and Pergolesi - 2020 - The impact of columnar file formats on SQL‐on‐hado.pdf:application/pdf},
}

@inproceedings{klang_wikiparq_2016,
	title = {{WikiParq}: {A} {Tabulated} {Wikipedia} {Resource} {Using} the {Parquet} {Format}},
	shorttitle = {{WikiParq}},
	url = {https://lup.lub.lu.se/record/5e2a7be2-9bc2-4d72-bec3-618939d0b729},
	abstract = {Wikipedia has become one of the most popular resources in natural language processing and it is used in quantities of applications. However, Wikipedia requires a substantial pre-processing step before it can be used. For instance, its set of nonstandardized annotations, referred to as the wiki markup, is language-dependent and needs specific parsers from language to language, for English, French, Italian, etc. In addition, the intricacies of the different Wikipedia resources: main article text, categories, wikidata, infoboxes, scattered into the article document or in different files make it difficult to have global view of this outstanding resource. In this paper, we describe WikiParq, a unified format based on the Parquet standard to tabulate and package the Wikipedia corpora. In combination with Spark, a map-reduce computing framework, and the SQL query language, WikiParq makes it much easier to write database queries to extract specific information or subcorpora from Wikipedia, such as all the first paragraphs of the articles in French, or all the articles on persons in Spanish, or all the articles on persons that have versions in French, English, and Spanish. WikiParq is available in six language versions and is potentially extendible to all the languages of Wikipedia. The WikiParq files are downloadable as tarball archives from this location: http://semantica.cs.lth.se/wikiparq/.},
	language = {eng},
	urldate = {2022-09-03},
	author = {Klang, Marcus and Nugues, Pierre},
	year = {2016},
	note = {Book Title: Proceedings of the 10th International Conference on Language Resources and Evaluation (LREC 2016)},
	keywords = {Computer and Information Science, Data- och informationsvetenskap (Datateknik), Language Technology (Computational Linguistics), Natural Sciences, Naturvetenskap, Språkteknologi (språkvetenskaplig databehandling)},
	pages = {4141--},
}

@article{blomer_quantitative_2018,
	title = {A quantitative review of data formats for {HEP} analyses},
	volume = {1085},
	issn = {1742-6596},
	url = {https://doi.org/10.1088/1742-6596/1085/3/032020},
	doi = {10.1088/1742-6596/1085/3/032020},
	abstract = {The analysis of High Energy Physics (HEP) data sets often takes place outside the realm of experiment frameworks and central computing workflows, using carefully selected “n-tuples” or Analysis Object Data (AOD) as a data source. Such n-tuples or AODs may comprise data from tens of millions of events and grow to hundred gigabytes or a few terabytes in size. They are typically small enough to be processed by an institute’s cluster or even by a single workstation. N-tuples and AODs are often stored in the ROOT file format, in an array of serialized C++ objects in columnar storage layout. In recent years, several new data formats emerged from the data analytics industry. We provide a quantitative comparison of ROOT and other popular data formats, such as Apache Parquet, Apache Avro, Google Protobuf, and HDF5. We compare speed, read patterns, and usage aspects for the use case of a typical LHC end-user n-tuple analysis. The performance characteristics of the relatively simple n-tuple data layout also provides a basis for understanding performance of more complex and nested data layouts. From the benchmarks, we derive performance tuning suggestions both for the use of the data formats and for the ROOT (de-)serialization code.},
	language = {en},
	urldate = {2022-09-03},
	journal = {Journal of Physics: Conference Series},
	author = {Blomer, J.},
	month = sep,
	year = {2018},
	note = {Publisher: IOP Publishing},
	pages = {032020},
	file = {IOP Full Text PDF:C\:\\Users\\Steven\\Zotero\\storage\\XRQ83B59\\Blomer - 2018 - A quantitative review of data formats for HEP anal.pdf:application/pdf},
}

@article{kuhring_i_2019,
	title = {I can't believe it's not (only) software! bionic distributed storage for {Parquet} files},
	volume = {12},
	issn = {2150-8097},
	url = {http://doi.org/10.14778/3352063.3352079},
	doi = {10.14778/3352063.3352079},
	abstract = {There is a steady increase in the size of data stored and processed as part of data science applications, leading to bottlenecks and inefficiencies at various layers of the stack. One way of reducing such bottlenecks and increasing energy efficiency is by tailoring the underlying distributed storage solution to the application domain, using resources more efficiently. We explore this idea in the context of a popular column-oriented storage format used in big data workloads, namely Apache Parquet. Our prototype uses an FPGA-based storage node that offers high bandwidth data deduplication and a companion software library that exposes an API for Parquet file access. This way the storage node remains general purpose and could be shared by applications from different domains, while, at the same time, benefiting from deduplication well suited to Apache Parquet files and from selective reads of columns in the file. In this demonstration we show, on the one hand, that by relying on the FPGA's dataflow processing model, it is possible to implement in-line deduplication without increasing latencies significantly or reducing throughput. On the other hand, we highlight the benefits of implementing the application-specific aspects in a software library instead of FPGA circuits and how this enables, for instance, regular data science frameworks running in Python to access the data on the storage node and to offload filtering operations.},
	number = {12},
	urldate = {2022-09-03},
	journal = {Proceedings of the VLDB Endowment},
	author = {Kuhring, Lucas and István, Zsolt},
	month = aug,
	year = {2019},
	pages = {1838--1841},
	file = {Full Text PDF:C\:\\Users\\Steven\\Zotero\\storage\\B838ZVZW\\Kuhring and István - 2019 - I can't believe it's not (only) software! bionic d.pdf:application/pdf},
}

@misc{howarth_30_2022,
	title = {30+ {Incredible} {Big} {Data} {Statistics} (2022)},
	url = {https://explodingtopics.com/blog/big-data-stats},
	abstract = {Our collection of statistics around the big data space (regularly updated).},
	language = {en},
	urldate = {2022-09-10},
	journal = {Exploding Topics},
	author = {Howarth, Josh},
	month = feb,
	year = {2022},
	file = {Snapshot:C\:\\Users\\Steven\\Zotero\\storage\\JELHKJSF\\big-data-stats.html:text/html},
}

@inproceedings{36632,
title	= {Dremel: Interactive Analysis of Web-Scale Datasets},
author	= {Sergey Melnik and Andrey Gubarev and Jing Jing Long and Geoffrey Romer and Shiva Shivakumar and Matt Tolton and Theo Vassilakis},
year	= {2010},
URL	= {http://www.vldb2010.org/accept.htm},
booktitle	= {Proc. of the 36th Int'l Conf on Very Large Data Bases},
pages	= {330-339}
}

@incollection{vohra_apache_2016_avro,
	address = {Berkeley, CA},
	title = {Apache {Avro}},
	isbn = {978-1-4842-2199-0},
	url = {https://doi.org/10.1007/978-1-4842-2199-0_7},
	abstract = {Apache Avro is a compact binary data serialization format providing varied data structures. Avro uses JSON notation schemas to serialize/deserialize data. Avro data is stored in a container file (an .avro file) and its schema (the .avsc file) is stored with the data file. Unlike some other similar systems such as Protocol buffers, Avro does not require code generation and uses dynamic typing. Data is untagged because the schema is accompanied with the data, resulting in a compact data file. Avro supports versioning; different versions (having different columns) of Avro data files may coexist along with their schemas. Another benefit of Avro is interoperability with other languages because of its efficient binary format. The Apache Hadoop ecosystem supports Apache Avro in several of its projects. Apache Hive provides support to store a table as Avro. The Apache sqoop import command supports importing relational data to an Avro data file. Apache Flume supports Avro as a source and sink type.},
	language = {en},
	urldate = {2022-09-12},
	booktitle = {Practical {Hadoop} {Ecosystem}: {A} {Definitive} {Guide} to {Hadoop}-{Related} {Frameworks} and {Tools}},
	publisher = {Apress},
	author = {Vohra, Deepak},
	editor = {Vohra, Deepak},
	year = {2016},
	doi = {10.1007/978-1-4842-2199-0_7},
	keywords = {Category String, Code String, External Table, Heap Space, Type String},
	pages = {303--323},
}


@techreport{shafranovich_common_2005,
	type = {Request for {Comments}},
	title = {Common {Format} and {MIME} {Type} for {Comma}-{Separated} {Values} ({CSV}) {Files}},
	url = {https://datatracker.ietf.org/doc/rfc4180},
	abstract = {This RFC documents the format used for Comma-Separated Values (CSV) files and registers the associated MIME type "text/csv". This memo provides information for the Internet community.},
	number = {RFC 4180},
	urldate = {2022-09-12},
	institution = {Internet Engineering Task Force},
	author = {Shafranovich, Yakov},
	month = oct,
	year = {2005},
	doi = {10.17487/RFC4180},
	note = {Num Pages: 8},
	file = {Full Text PDF:C\:\\Users\\Steven\\Zotero\\storage\\Q6EQ3GBF\\Shafranovich - 2005 - Common Format and MIME Type for Comma-Separated Va.pdf:application/pdf},
}

@misc{noauthor_xlsx_2022,
	type = {web page},
	title = {{XLSX} {Transitional} ({Office} {Open} {XML}), {ISO} 29500:2008-2016, {ECMA}-376, {Editions} 1-5},
	copyright = {Text is U.S. government work},
	shorttitle = {{XLSX} {Transitional} ({Office} {Open} {XML}), {ISO} 29500},
	url = {https://www.loc.gov/preservation/digital/formats/fdd/fdd000398.shtml},
	abstract = {Format Description for XLSX/OOXML\_2012 -- A format for spreadsheet documents in the Open Office XML (ISO 29500) format family, developed originally by Microsoft and used as the default format used by Microsoft Excel 2007 and later versions. Specified in parts 1 and 4 of ISO/IEC 29500, Information technology -- Document description and processing languages -- Office Open XML File Formats (OOXML).},
	language = {eng},
	urldate = {2022-09-12},
	month = may,
	year = {2022},
	file = {Snapshot:C\:\\Users\\Steven\\Zotero\\storage\\FGH65JCG\\fdd000398.html:text/html},
}

@misc{noauthor_covid-19_nodate,
	title = {{COVID}-19 {Open} {Data} — {Google} {Health}},
	url = {https://health.google.com/covid-19/open-data/},
	urldate = {2022-09-13},
	file = {COVID-19 Open Data — Google Health:C\:\\Users\\Steven\\Zotero\\storage\\CJL8YW3N\\open-data.html:text/html},
}
@inbook{floratou2018columnar,
author = {Floratou, Avrilia},
title = {Columnar Storage Formats},
booktitle = {Encyclopedia of Big Data Technologies},
year = {2018},
month = {May},
abstract = {Fast analytics over Hadoop data has gained significant traction over the last few years, as multiple enterprises are using Hadoop to store data coming from various sources including operational systems, sensors and mobile devices, and web applications. Various Big Data frameworks have been developed to support fast analytics on top of this data and to provide insights in near real time.

A crucial aspect in delivering high performance in such large-scale environments is the underlying data layout. Most Big Data frameworks are designed to operate on top of data stored in various formats, and they are extensible enough to incorporate new data formats. Over the years, a plethora of open-source data formats have been designed to support the needs of various applications. These formats can be row or column oriented and can support various forms of serialization and compression. The columnar data formats are a popular choice for fast analytics workloads. As opposed to row-oriented storage, columnar storage can significantly reduce the amount of data fetched from disk by allowing access to only the columns that are relevant for the particular query or workload. Moreover, columnar storage combined with efficient encoding and compression techniques can drastically reduce the storage requirements without sacrificing query performance.

Column-oriented storage has been successfully incorporated in both disk-based and memory-based relational databases that target OLAP workloads (Vertica 2017). In the context of Big Data frameworks, the first works on columnar storage for data stored in HDFS (Apache Hadoop HDFS 2017) have appeared around 2011 (He et al. 2011; Floratou et al. 2011). Over the years, multiple proposals have been made to satisfy the needs of various applications and to address the increasing data volume and complexity. These discussions resulted in the creation of two popular columnar formats, namely, the Parquet (Apache Parquet 2017) and ORC (Apache ORC 2017) file formats. These formats are both open-source and are currently supported by multiple proprietary and open-source Big Data frameworks. Apart from columnar organization, the formats provide efficient encoding and compression techniques and incorporate various statistics that enable predicate pushdown which can further improve the performance of analytics workloads.

In this article, we first present the major works in disk-based columnar storage in the context of Big Data systems and Hadoop data. We then provide a detailed description of the Parquet and ORC file formats which are the most widely adopted columnar formats in current Big Data frameworks. We conclude the article by highlighting the similarities and differences of these two formats.},
publisher = {Springer},
url = {https://www.microsoft.com/en-us/research/publication/columnar-storage-formats/},
}

@misc{jack_future_nodate,
	title = {The future of column-oriented data processing with {Arrow} and {Parquet} - ppt download},
	url = {https://slideplayer.com/slide/14421705/},
	abstract = {Apache PMCs: Arrow, Calcite, Drill, Incubator  Jacques Julien Le CTO of Dremio Apache member VP Apache Arrow Apache PMCs: Arrow, Calcite, Drill, Incubator Principal Architect at Dremio Formerly Tech Lead at Twitter on Data Platforms. Creator of Parquet Apache member Apache PMCs: Arrow, Incubator, Kudu, Pig, Parquet},
	urldate = {2022-10-04},
	author = {Jack, Jacques Nadeau, Dremio, Julien Le Dem, Read, Joe},
	file = {Snapshot:C\:\\Users\\Steven\\Zotero\\storage\\XJ8XETRF\\14421705.html:text/html},
}


@misc{grondin_bitflip_2022,
	title = {bitflip},
	copyright = {MIT},
	url = {https://github.com/aybabtme/bitflip},
	abstract = {Flip bits in files.},
	urldate = {2022-10-04},
	author = {Grondin, Antoine},
	month = feb,
	year = {2022},
	note = {original-date: 2020-04-09T19:05:11Z},
}


@article{bexell_responsibility_2017,
	title = {Responsibility and the {United} {Nations}’ {Sustainable} {Development} {Goals}},
	volume = {44},
	issn = {0803-9410},
	url = {https://doi.org/10.1080/08039410.2016.1252424},
	doi = {10.1080/08039410.2016.1252424},
	abstract = {This article asks what key concerns emerge from the way responsibility is framed in United Nations summit documents on the Sustainable Development Goals (SDGs) adopted in 2015. Our conceptual framework serves to make the study of SDG responsibility more systematic by distinguishing three main senses of responsibility: cause, obligation, and accountability. The framework structures our analysis of two SDG summit documents, Transforming Our World: The 2030 Agenda for Sustainable Development and the Addis Ababa Action Agenda. The article shows, first, that the causal sense of responsibility is hidden between the lines in paragraphs on poverty, debt and environmental issues. As a consequence, root causes of problems might not be appropriately addressed. Second, SDG summit documents deal predominantly with responsibility in the sense of obligation. We raise concerns with repeated consideration for national circumstances and with vague obligations for non-governmental actors. Third, with regard to accountability, we stress that quantitative indicators have unintended steering effects both before and beyond the review phase. The focus on indicators risks shadowing broader obligations, such as international human rights. In all its three senses, responsibility in key SDG documents remains state-centric with great room for state sovereignty, self-regulation and respect for national circumstances. Our framework is useful also in showing that the three senses of responsibility build on each other and that engagement with responsibility provides fruitful ground for further research.},
	number = {1},
	urldate = {2021-02-25},
	journal = {Forum for Development Studies},
	author = {Bexell, Magdalena and Jönsson, Kristina},
	month = jan,
	year = {2017},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/08039410.2016.1252424},
	keywords = {accountability, responsibility, sustainable development goals, United Nations},
	pages = {13--29},
	file = {Full Text PDF:/Users/emilstahl/Zotero/storage/JTP3J9LR/Bexell och Jönsson - 2017 - Responsibility and the United Nations’ Sustainable.pdf:application/pdf;Snapshot:/Users/emilstahl/Zotero/storage/PIENM9H2/08039410.2016.html:text/html},
}



@inproceedings{hakansson_portal_2013,
	title = {Portal of {Research} {Methods} and {Methodologies} for {Research} {Projects} and {Degree} {Projects}},
	url = {http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-136960},
	abstract = {Research methods and methodologies are extremelyimportant when conducting research and degree projects. Theuse and application of the methods and methodologies areconsidered to be “necessarily vici ...},
	language = {eng},
	urldate = {2021-05-25},
	publisher = {CSREA Press U.S.A},
	author = {Håkansson, Anne},
	year = {2013},
	pages = {67--73},
	file = {Full Text PDF:/Users/emilstahl/Zotero/storage/UYXJSJCZ/Håkansson - 2013 - Portal of Research Methods and Methodologies for R.pdf:application/pdf;Snapshot:/Users/emilstahl/Zotero/storage/SQ7PHZGA/record.html:text/html},
}



@misc{noauthor_google_2022,
	title = {Google {Colab}},
	url = {https://research.google.com/colaboratory/faq.html},
	urldate = {2022-10-26},
	journal = {Google},
	month = jun,
	year = {2022},
	file = {Google Colab:/Users/emilstahl/Zotero/storage/DUA9PM6X/faq.html:text/html},
}




@misc{noauthor_state_2022,
	title = {State of {IoT} 2022: {Number} of connected {IoT} devices growing 18\% to 14.4 billion globally},
	shorttitle = {State of {IoT} 2022},
	url = {https://iot-analytics.com/number-connected-iot-devices/},
	abstract = {IoT connections market update—May 2022 The chip shortage continues to slow the Internet of Things (IoT) market recovery, according to our latest State of IoT—Spring 2022 report, released in May},
	language = {en-US},
	urldate = {2022-10-27},
	journal = {IoT Analytics},
	month = may,
	year = {2022},
	file = {Snapshot:C\:\\Users\\Steven\\Zotero\\storage\\7DTYV293\\number-connected-iot-devices.html:text/html},
}



@article{cao_data_2017,
	title = {Data {Science}: {A} {Comprehensive} {Overview}},
	volume = {50},
	issn = {0360-0300},
	shorttitle = {Data {Science}},
	url = {https://doi.org/10.1145/3076253},
	doi = {10.1145/3076253},
	abstract = {The 21st century has ushered in the age of big data and data economy, in which data DNA, which carries important knowledge, insights, and potential, has become an intrinsic constituent of all data-based organisms. An appropriate understanding of data DNA and its organisms relies on the new field of data science and its keystone, analytics. Although it is widely debated whether big data is only hype and buzz, and data science is still in a very early phase, significant challenges and opportunities are emerging or have been inspired by the research, innovation, business, profession, and education of data science. This article provides a comprehensive survey and tutorial of the fundamental aspects of data science: the evolution from data analysis to data science, the data science concepts, a big picture of the era of data science, the major challenges and directions in data innovation, the nature of data analytics, new industrialization and service opportunities in the data economy, the profession and competency of data education, and the future of data science. This article is the first in the field to draw a comprehensive big picture, in addition to offering rich observations, lessons, and thinking about data science and analytics.},
	number = {3},
	urldate = {2022-10-26},
	journal = {ACM Computing Surveys},
	author = {Cao, Longbing},
	month = jun,
	year = {2017},
	keywords = {Big data, advanced analytics, big data analytics, computing, data analysis, data analytics, data DNA, data economy, data education, data engineering, data industry, data innovation, data profession, data science, data scientist, data service, informatics, statistics},
	pages = {43:1--43:42},
	file = {Full Text PDF:/Users/emilstahl/Zotero/storage/3VZWDJCT/Cao - 2017 - Data Science A Comprehensive Overview.pdf:application/pdf},
}

