\section{Introduction}
\label{sect:introduction}
Data science pipelines process vast amounts of data stored in various data formats. In the field of the \gls{IoT} alone, there are billions of sensors and computing devices that are collecting and analyzing large amounts of personal data \cite{noauthor_state_2022}. For pipelines to be high performing, it is important to choose the right data format for your application. Since different data formats are implemented using different technologies, they may differ in how they handle different sizes of data \cite{cao_data_2017}. However, there are currently no conventions or general knowledge regarding when to use a particular data format for a given data size to optimize for performance, file stability, and file size. In this work, we are going to investigate how data science pipelines can be optimized by choosing the correct format when processing textual time series data. 


\subsection{Background}
\label{sec:introbac}
The modern society produces a vast amount of data every day across many different types of industries and pipelines. Until processed, these data are merely bits placed on a storage medium. The field of data science is focused on extracting useful knowledge from such data in order to gain a deeper understanding of a given field. Data can be stored in various formats such as csv, xlsx, Parquet, and Avro. Processing data requires many operations to be applied, such as reading, writing, and other mathematical operations. The value that data science brings places heavy demands on the performance and stability of the data formats used in data processing and storage pipelines. Related work has been done by Plase et al., who in their work "\textit{A comparison of HDFS compact data formats: Avro versus Parquet}" have benchmarked the amount of disk space these formats consume, as well as their performance in the \gls{HEP} analysis, which uses a lot of numerical data. However, the work does not benchmark read-and-write speed, file stability, or performance on time series text data. Understanding the performance advantages of different data formats in terms of read and write speed when working with time series text data is therefore of paramount importance in the data science community\cite{plase_comparison_2017, cao_data_2017}.

\subsection{Problem}
\label{sect:problem}
Analyzing the performance and stability of different data formats is key to addressing the scalability and performance challenges in which data science pipelines operate. Today, there exist more than 44 zeta bytes of data with approximately 2.5 Quintilian bytes of data being generated each day \cite{howarth_30_2022}. Data science pipelines must be highly optimized performance-wise to be able to process data at sufficient speeds and to ensure stability as well as job efficiency \cite{cao_data_2017}. Since different data formats are implemented using different technologies, they may differ in how they handle different sizes of data, which affects the performance when processing and storing the data. There are currently no conventions for when to use a particular data format for a given data size to optimize performance, file stability, and file size. Since csv, xlsx, Parquet, and Avro are all widely used in data science, it is important to have an understanding of their behavior when processing and storing data \cite{cao_data_2017}.

\subsection{Purpose}
\label{sec:purpose}
This paper aims to give the interested reader guidelines regarding how to choose a data format when working with textual time series data. The purpose of this work is to analyze the performance of the csv, xlsx, Parquet, and Avro data formats by
quantifying their read/write speed, file stability, and file size to determine which data format is the most optimal when working with textual time series data. By analyzing the performance of these data formats under different pipeline conditions, such as file size, one can use this work to tailor data science pipelines to optimize performance. The results are potentially of interest to general data scientists, stock market traders, developers of big data frameworks, and the open-source community.

\clearpage
\subsection{Research questions and hypothesis}
Considering the background and problem discussion, this work will treat the following questions. 
\label{sect:questions}
\begin{itemize}
    \item \textbf{RQ1} How do the csv, xlsx, Parquet, and Avro data formats differ in terms of read and write performance when processing textual time series data?
    \item \textbf{RQ2} How do the csv, xlsx, Parquet, and Avro data formats differ in terms of resilience against file errors due to random bit flips when storing textual time series data?
    \item \textbf{RQ3} How do the csv, xlsx, Parquet, and Avro data formats differ in terms of file size when storing textual time series data?
\end{itemize}

With this work, we expect there to be an advantageous data format for a given file size when working with textual time series data. The row-based Avro format should provide advantages in performance with regard to simple operations such as read/write speeds when compared to csv, xlsx, and Parquet. For larger time series data sets, the Apache Parquet format should provide better performance. Regarding file stability we expect csv to be more resilient against file errors due to that it is uncompressed.


\subsection{Benefits, Sustainability, and Ethics}
\label{sect:benefits}
Many possible benefits may result from understanding the performance and stability of different data formats in data science. Regarding sustainability, the biggest contribution in this matter would be to reduce the energy consumption of data science pipelines. Smaller file sizes result in less network bandwidth being used, as well as less waiting time for the cluster to load and process data. This would hopefully be able to free up more machines which results in less energy being consumed and combined with works such as "Green network control" it would have a meaningful environmental impact \cite{cao_data_2017}. Sustainable energy, meaning less consumption, is one of the Sustainable Development Goals created by the United Nations \cite{bexell_responsibility_2017}. Therefore, this work could result in positive sustainability impacts. Regarding ethics, there exist several areas that are interesting from an ethics perspective. For example, showing limitations in how a specific data format or software performs and functions may cause harm to the developers behind it. This may deter people from using it which may cause harm to the copyright holder or third-party software that relies on the data format in question. However, benchmarking is widely accepted in the software community and is often welcomed by the users and developers behind the software that is being tested. Therefore, we argue that there are no major ethical problems that deter us from researching this area \cite{bexell_responsibility_2017}. 

  
\subsection{Methodology}
\label{sect:intromethod}
The decision of methodology is important in a scientific study. In this work, the research method used is defined as \textit{experimental} since the method focuses on finding aspects that affect the choice of data format when working with textual time series data \cite{hakansson_portal_2013}. Moreover, the research is also \textit{applied} since it makes use of the code for the data formats that we benchmark as well as libraries and existing methodologies, which means that the methodology is based on work performed by other researchers and developers \cite{hakansson_portal_2013, blomer_quantitative_2018}. With this in mind, the methodology used to perform this work consists of the choice and configuration of a benchmarking environment, a method for performing the experiments, and an evaluation method to analyze and evaluate the system itself. The chosen method should fulfill the purpose and goal of this thesis, which is to analyze the performance of different data formats \cite{hakansson_portal_2013}.

\subsection{Delimitations}
\label{sect:delimit}
Due to the scope of this project as well as our knowledge and available resources, a set of delimitations are needed for this work. The delimitations can be constricted to those regarding the application of the data formats to be benchmarked. As the application may influence the choice of data format, we have decided to delimit our work to only benchmark the data formats when working with textual time series data using the Pandas Python library. 

\subsection{Outline}
\label{sect:outline}
This paper is organized as follows. Chapter \ref{sect:introduction} gives an introduction to the work where the background is briefly explained. In addition to this, the purpose, the goal, and the methodology used are shortly explained along with its connection to ethics and sustainability. Chapter \ref{sect:theoryframework} is a recess of the theoretical framework and literature used in our work. Further on, Chapter \ref{sec:method} describes the methodologies and methods used to perform the work, which includes the benchmarking environment and metrics. Next, Chapter \ref{sec:results} presents the results of the performed benchmarks, mainly focusing on the performance and stability of the data formats and how these differ in-between file sizes. Chapter \ref{sec:discussion} analyses and discusses the results. In other words, what the results mean and why they look the way that they do. We also discuss how the experiments could be changed to improve the accuracy and reliability of the benchmarks. Finally, Chapter \ref{sec:conclusion} presents a conclusion of the work in relation to its purpose and goal, as well as potential future work that can be performed in order to improve the conclusions of this work.
